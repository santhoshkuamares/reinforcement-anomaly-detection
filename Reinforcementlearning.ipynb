{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/santhoshkuamares/reinforcement-anomaly-detection.git\n",
        "%cd reinforcement-anomaly-detection\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1tArERcR3PL",
        "outputId": "e970db8d-dea1-4760-88b5-69504877e261"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'reinforcement-anomaly-detection'...\n",
            "warning: You appear to have cloned an empty repository.\n",
            "/content/reinforcement-anomaly-detection/reinforcement-anomaly-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Imports**\n",
        "\n",
        "This section imports libraries for data handling (pandas, numpy), preprocessing (scikit-learn), and deep reinforcement learning (PyTorch). It sets up everything needed for loading data, training the RL model, and evaluating results."
      ],
      "metadata": {
        "id": "Y6oisM8wLywv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JcfA78OYrVxb"
      },
      "outputs": [],
      "source": [
        "#Step 1: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "sns.set(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Load Data**\n",
        "\n",
        "The AI4I dataset is loaded and cleaned by dropping irrelevant columns (UDI, Product ID). New features like temperature difference and power are engineered to better capture machine failure signals."
      ],
      "metadata": {
        "id": "DokaReTtMYFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Step 2: Load Data\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'\n",
        "data = pd.read_csv(url)\n",
        "data_clean = data.drop(['UDI', 'Product ID'], axis=1)\n"
      ],
      "metadata": {
        "id": "3dlRk5G_r2cF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eda\n",
        "print(data_clean.shape)\n",
        "print(data_clean.head())\n",
        "print(data_clean.info())\n",
        "print(data_clean.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SAJz2jG9ohv",
        "outputId": "3c42a016-9de6-41e7-9f66-ae98ff9bc8a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 12)\n",
            "  Type  Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
            "0    M                298.1                    308.6                    1551   \n",
            "1    L                298.2                    308.7                    1408   \n",
            "2    L                298.1                    308.5                    1498   \n",
            "3    L                298.2                    308.6                    1433   \n",
            "4    L                298.2                    308.7                    1408   \n",
            "\n",
            "   Torque [Nm]  Tool wear [min]  Machine failure  TWF  HDF  PWF  OSF  RNF  \n",
            "0         42.8                0                0    0    0    0    0    0  \n",
            "1         46.3                3                0    0    0    0    0    0  \n",
            "2         49.4                5                0    0    0    0    0    0  \n",
            "3         39.5                7                0    0    0    0    0    0  \n",
            "4         40.0                9                0    0    0    0    0    0  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 12 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   Type                     10000 non-null  object \n",
            " 1   Air temperature [K]      10000 non-null  float64\n",
            " 2   Process temperature [K]  10000 non-null  float64\n",
            " 3   Rotational speed [rpm]   10000 non-null  int64  \n",
            " 4   Torque [Nm]              10000 non-null  float64\n",
            " 5   Tool wear [min]          10000 non-null  int64  \n",
            " 6   Machine failure          10000 non-null  int64  \n",
            " 7   TWF                      10000 non-null  int64  \n",
            " 8   HDF                      10000 non-null  int64  \n",
            " 9   PWF                      10000 non-null  int64  \n",
            " 10  OSF                      10000 non-null  int64  \n",
            " 11  RNF                      10000 non-null  int64  \n",
            "dtypes: float64(3), int64(8), object(1)\n",
            "memory usage: 937.6+ KB\n",
            "None\n",
            "       Air temperature [K]  Process temperature [K]  Rotational speed [rpm]  \\\n",
            "count         10000.000000             10000.000000            10000.000000   \n",
            "mean            300.004930               310.005560             1538.776100   \n",
            "std               2.000259                 1.483734              179.284096   \n",
            "min             295.300000               305.700000             1168.000000   \n",
            "25%             298.300000               308.800000             1423.000000   \n",
            "50%             300.100000               310.100000             1503.000000   \n",
            "75%             301.500000               311.100000             1612.000000   \n",
            "max             304.500000               313.800000             2886.000000   \n",
            "\n",
            "        Torque [Nm]  Tool wear [min]  Machine failure           TWF  \\\n",
            "count  10000.000000     10000.000000     10000.000000  10000.000000   \n",
            "mean      39.986910       107.951000         0.033900      0.004600   \n",
            "std        9.968934        63.654147         0.180981      0.067671   \n",
            "min        3.800000         0.000000         0.000000      0.000000   \n",
            "25%       33.200000        53.000000         0.000000      0.000000   \n",
            "50%       40.100000       108.000000         0.000000      0.000000   \n",
            "75%       46.800000       162.000000         0.000000      0.000000   \n",
            "max       76.600000       253.000000         1.000000      1.000000   \n",
            "\n",
            "                HDF           PWF           OSF          RNF  \n",
            "count  10000.000000  10000.000000  10000.000000  10000.00000  \n",
            "mean       0.011500      0.009500      0.009800      0.00190  \n",
            "std        0.106625      0.097009      0.098514      0.04355  \n",
            "min        0.000000      0.000000      0.000000      0.00000  \n",
            "25%        0.000000      0.000000      0.000000      0.00000  \n",
            "50%        0.000000      0.000000      0.000000      0.00000  \n",
            "75%        0.000000      0.000000      0.000000      0.00000  \n",
            "max        1.000000      1.000000      1.000000      1.00000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**About Feature engineered**\n",
        "\n",
        "I created Temp_Diff (process – air temperature) to capture overheating risk, since large gaps between ambient and process heat can signal machine stress and failure.\n",
        "\n",
        "You engineered Power (torque × rotational speed) using physics to reflect the machine’s actual workload, which is more informative than torque or speed alone.\n"
      ],
      "metadata": {
        "id": "PehygTMzMqnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#featured engineered\n",
        "data_clean['Temp_Diff'] = data_clean['Process temperature [K]'] - data_clean['Air temperature [K]']\n",
        "data_clean['Power'] = data_clean['Torque [Nm]'] * data_clean['Rotational speed [rpm]']\n",
        "\n",
        "features = data_clean.drop(['Machine failure', 'TWF','HDF','PWF','OSF','RNF'], axis=1)\n",
        "labels = data_clean['HDF']\n",
        "\n",
        "categorical_cols = ['Type']\n",
        "numerical_cols = features.select_dtypes(include=[np.number]).columns.tolist()\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
        "    ])\n",
        "\n"
      ],
      "metadata": {
        "id": "UyStJtEWr6wP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = preprocessor.fit_transform(features)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bnq4aHaa-yXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
        "X_test = X_test.toarray() if hasattr(X_test, 'toarray') else X_test\n",
        "y_train, y_test = y_train.values, y_test.values"
      ],
      "metadata": {
        "id": "QpJCnIA9r9Ka"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Oversample anomalies in training**\n",
        "\n",
        "Since failures are rare, anomalies are oversampled to balance the dataset. This prevents the model from being biased toward predicting only normal cases."
      ],
      "metadata": {
        "id": "H05Dp29lMjvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Oversample anomalies in training\n",
        "anomaly_idx = np.where(y_train == 1)[0]\n",
        "normal_idx = np.where(y_train == 0)[0]\n",
        "anomaly_samples = np.random.choice(anomaly_idx, size=len(normal_idx), replace=True)\n",
        "\n",
        "X_train_bal = np.vstack([X_train[normal_idx], X_train[anomaly_samples]])\n",
        "y_train_bal = np.hstack([y_train[normal_idx], y_train[anomaly_samples]])\n",
        "\n",
        "# Shuffle\n",
        "idx = np.random.permutation(len(y_train_bal))\n",
        "X_train, y_train = X_train_bal[idx], y_train_bal[idx]"
      ],
      "metadata": {
        "id": "CIC1cBZxr_0N"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: RL Environment**\n",
        "\n",
        "A custom environment (AnomalyEnv) simulates the predictive maintenance problem where the agent chooses actions (normal vs. anomaly). Rewards are shaped to strongly penalize false alarms and missed failures, guiding the agent toward better anomaly detection."
      ],
      "metadata": {
        "id": "0tTVDpzfMsFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: RL Environment\n",
        "class AnomalyEnv:\n",
        "    def __init__(self, data, labels):\n",
        "        self.data, self.labels = data, labels\n",
        "        self.current = 0\n",
        "        self.max_steps = len(data)\n",
        "    def reset(self):\n",
        "        self.current = 0\n",
        "        return self.data[0]\n",
        "    def step(self, action):\n",
        "        true = self.labels[self.current]\n",
        "        # Reward shaping\n",
        "        if action == 1 and true == 1: reward = 6   # TP\n",
        "        elif action == 1 and true == 0: reward = -5  # FP (stronger penalty)\n",
        "        elif action == 0 and true == 1: reward = -6  # FN\n",
        "        else: reward = 2   # TN\n",
        "        self.current += 1\n",
        "        done = self.current >= self.max_steps\n",
        "        next_state = self.data[self.current] if not done else np.zeros_like(self.data[0])\n",
        "        return next_state, reward, done"
      ],
      "metadata": {
        "id": "MiPbLY-PDC0z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Tiny DQN**\n",
        "\n",
        "A small Deep Q-Network (DQN) is defined with two layers. It learns to approximate the best action (detect anomaly or not) given machine sensor data."
      ],
      "metadata": {
        "id": "BcXhbdr3M2Mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Tiny DQN\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 32)\n",
        "        self.fc2 = nn.Linear(32, action_size)\n",
        "    def forward(self, x):\n",
        "        return self.fc2(torch.relu(self.fc1(x)))\n"
      ],
      "metadata": {
        "id": "Kzn0MhNAsDSl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Training loop**\n",
        "\n",
        "The agent interacts with the environment for multiple episodes, choosing actions, collecting rewards, and storing experiences. The DQN is trained using replay memory and periodically updated with a target network to stabilize learning."
      ],
      "metadata": {
        "id": "n4F0d4o5M7i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Step 6: Training loop\n",
        "state_size = X_train.shape[1]\n",
        "action_size = 2\n",
        "model = DQN(state_size, action_size)\n",
        "target = DQN(state_size, action_size)\n",
        "target.load_state_dict(model.state_dict())\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "gamma = 0.9\n",
        "epsilon, eps_min, eps_decay = 1.0, 0.05, 0.98\n",
        "memory = deque(maxlen=1000)\n",
        "batch_size = 16\n",
        "episodes = 100   # a bit longer but still lightweight\n",
        "\n",
        "env = AnomalyEnv(X_train, y_train)\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = torch.FloatTensor(env.reset()).unsqueeze(0)\n",
        "    done, total_reward = False, 0\n",
        "    while not done:\n",
        "        if random.random() < epsilon:\n",
        "            action = random.randrange(action_size)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                action = torch.argmax(model(state)).item()\n",
        "        next_state, reward, done = env.step(action)\n",
        "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "        memory.append((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if len(memory) >= batch_size:\n",
        "            batch = random.sample(memory, batch_size)\n",
        "            states = torch.cat([b[0] for b in batch])\n",
        "            actions = torch.LongTensor([b[1] for b in batch])\n",
        "            rewards = torch.FloatTensor([b[2] for b in batch])\n",
        "            next_states = torch.cat([b[3] for b in batch])\n",
        "            dones = torch.FloatTensor([b[4] for b in batch])\n",
        "\n",
        "            q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "            next_q = target(next_states).max(1)[0]\n",
        "            target_q = rewards + (1 - dones) * gamma * next_q\n",
        "            loss = nn.MSELoss()(q_values, target_q.detach())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    if ep % 10 == 0:\n",
        "        target.load_state_dict(model.state_dict())\n",
        "    epsilon = max(eps_min, epsilon * eps_decay)\n",
        "    print(f\"Episode {ep+1}/{episodes}, Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PEp0pkfR1yk",
        "outputId": "7f5215e3-2a38-49b5-a294-3a455ad88ccf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100, Reward: -10993, Epsilon: 0.98\n",
            "Episode 2/100, Reward: -10695, Epsilon: 0.96\n",
            "Episode 3/100, Reward: -9001, Epsilon: 0.94\n",
            "Episode 4/100, Reward: -6562, Epsilon: 0.92\n",
            "Episode 5/100, Reward: -5052, Epsilon: 0.90\n",
            "Episode 6/100, Reward: -5368, Epsilon: 0.89\n",
            "Episode 7/100, Reward: -3737, Epsilon: 0.87\n",
            "Episode 8/100, Reward: -2491, Epsilon: 0.85\n",
            "Episode 9/100, Reward: -1338, Epsilon: 0.83\n",
            "Episode 10/100, Reward: 986, Epsilon: 0.82\n",
            "Episode 11/100, Reward: 1928, Epsilon: 0.80\n",
            "Episode 12/100, Reward: 3331, Epsilon: 0.78\n",
            "Episode 13/100, Reward: 4713, Epsilon: 0.77\n",
            "Episode 14/100, Reward: 6219, Epsilon: 0.75\n",
            "Episode 15/100, Reward: 6208, Epsilon: 0.74\n",
            "Episode 16/100, Reward: 7760, Epsilon: 0.72\n",
            "Episode 17/100, Reward: 8970, Epsilon: 0.71\n",
            "Episode 18/100, Reward: 8511, Epsilon: 0.70\n",
            "Episode 19/100, Reward: 10855, Epsilon: 0.68\n",
            "Episode 20/100, Reward: 11560, Epsilon: 0.67\n",
            "Episode 21/100, Reward: 13517, Epsilon: 0.65\n",
            "Episode 22/100, Reward: 13473, Epsilon: 0.64\n",
            "Episode 23/100, Reward: 15124, Epsilon: 0.63\n",
            "Episode 24/100, Reward: 14922, Epsilon: 0.62\n",
            "Episode 25/100, Reward: 16602, Epsilon: 0.60\n",
            "Episode 26/100, Reward: 17475, Epsilon: 0.59\n",
            "Episode 27/100, Reward: 18830, Epsilon: 0.58\n",
            "Episode 28/100, Reward: 19336, Epsilon: 0.57\n",
            "Episode 29/100, Reward: 20708, Epsilon: 0.56\n",
            "Episode 30/100, Reward: 21397, Epsilon: 0.55\n",
            "Episode 31/100, Reward: 22991, Epsilon: 0.53\n",
            "Episode 32/100, Reward: 24656, Epsilon: 0.52\n",
            "Episode 33/100, Reward: 22634, Epsilon: 0.51\n",
            "Episode 34/100, Reward: 24582, Epsilon: 0.50\n",
            "Episode 35/100, Reward: 25481, Epsilon: 0.49\n",
            "Episode 36/100, Reward: 26385, Epsilon: 0.48\n",
            "Episode 37/100, Reward: 25960, Epsilon: 0.47\n",
            "Episode 38/100, Reward: 27542, Epsilon: 0.46\n",
            "Episode 39/100, Reward: 28298, Epsilon: 0.45\n",
            "Episode 40/100, Reward: 29223, Epsilon: 0.45\n",
            "Episode 41/100, Reward: 29117, Epsilon: 0.44\n",
            "Episode 42/100, Reward: 29677, Epsilon: 0.43\n",
            "Episode 43/100, Reward: 31507, Epsilon: 0.42\n",
            "Episode 44/100, Reward: 31825, Epsilon: 0.41\n",
            "Episode 45/100, Reward: 33492, Epsilon: 0.40\n",
            "Episode 46/100, Reward: 32684, Epsilon: 0.39\n",
            "Episode 47/100, Reward: 32934, Epsilon: 0.39\n",
            "Episode 48/100, Reward: 34163, Epsilon: 0.38\n",
            "Episode 49/100, Reward: 34731, Epsilon: 0.37\n",
            "Episode 50/100, Reward: 34690, Epsilon: 0.36\n",
            "Episode 51/100, Reward: 36809, Epsilon: 0.36\n",
            "Episode 52/100, Reward: 37186, Epsilon: 0.35\n",
            "Episode 53/100, Reward: 37044, Epsilon: 0.34\n",
            "Episode 54/100, Reward: 37345, Epsilon: 0.34\n",
            "Episode 55/100, Reward: 37662, Epsilon: 0.33\n",
            "Episode 56/100, Reward: 37382, Epsilon: 0.32\n",
            "Episode 57/100, Reward: 38262, Epsilon: 0.32\n",
            "Episode 58/100, Reward: 38723, Epsilon: 0.31\n",
            "Episode 59/100, Reward: 39741, Epsilon: 0.30\n",
            "Episode 60/100, Reward: 40031, Epsilon: 0.30\n",
            "Episode 61/100, Reward: 41372, Epsilon: 0.29\n",
            "Episode 62/100, Reward: 39866, Epsilon: 0.29\n",
            "Episode 63/100, Reward: 41723, Epsilon: 0.28\n",
            "Episode 64/100, Reward: 41948, Epsilon: 0.27\n",
            "Episode 65/100, Reward: 42488, Epsilon: 0.27\n",
            "Episode 66/100, Reward: 42677, Epsilon: 0.26\n",
            "Episode 67/100, Reward: 43134, Epsilon: 0.26\n",
            "Episode 68/100, Reward: 43765, Epsilon: 0.25\n",
            "Episode 69/100, Reward: 44396, Epsilon: 0.25\n",
            "Episode 70/100, Reward: 44308, Epsilon: 0.24\n",
            "Episode 71/100, Reward: 44534, Epsilon: 0.24\n",
            "Episode 72/100, Reward: 45809, Epsilon: 0.23\n",
            "Episode 73/100, Reward: 45798, Epsilon: 0.23\n",
            "Episode 74/100, Reward: 45218, Epsilon: 0.22\n",
            "Episode 75/100, Reward: 46111, Epsilon: 0.22\n",
            "Episode 76/100, Reward: 46447, Epsilon: 0.22\n",
            "Episode 77/100, Reward: 46415, Epsilon: 0.21\n",
            "Episode 78/100, Reward: 47115, Epsilon: 0.21\n",
            "Episode 79/100, Reward: 47164, Epsilon: 0.20\n",
            "Episode 80/100, Reward: 47270, Epsilon: 0.20\n",
            "Episode 81/100, Reward: 47524, Epsilon: 0.19\n",
            "Episode 82/100, Reward: 48471, Epsilon: 0.19\n",
            "Episode 83/100, Reward: 48462, Epsilon: 0.19\n",
            "Episode 84/100, Reward: 49287, Epsilon: 0.18\n",
            "Episode 85/100, Reward: 48390, Epsilon: 0.18\n",
            "Episode 86/100, Reward: 49104, Epsilon: 0.18\n",
            "Episode 87/100, Reward: 49523, Epsilon: 0.17\n",
            "Episode 88/100, Reward: 50493, Epsilon: 0.17\n",
            "Episode 89/100, Reward: 49804, Epsilon: 0.17\n",
            "Episode 90/100, Reward: 50611, Epsilon: 0.16\n",
            "Episode 91/100, Reward: 50818, Epsilon: 0.16\n",
            "Episode 92/100, Reward: 50784, Epsilon: 0.16\n",
            "Episode 93/100, Reward: 50776, Epsilon: 0.15\n",
            "Episode 94/100, Reward: 51829, Epsilon: 0.15\n",
            "Episode 95/100, Reward: 51546, Epsilon: 0.15\n",
            "Episode 96/100, Reward: 52413, Epsilon: 0.14\n",
            "Episode 97/100, Reward: 52464, Epsilon: 0.14\n",
            "Episode 98/100, Reward: 51959, Epsilon: 0.14\n",
            "Episode 99/100, Reward: 52742, Epsilon: 0.14\n",
            "Episode 100/100, Reward: 53009, Epsilon: 0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7: Evaluation with confidence threshold**\n",
        "\n",
        "The trained DQN is evaluated on test data with a stricter threshold to avoid false positives. Predictions are compared with ground truth using a classification report and confusion matrix."
      ],
      "metadata": {
        "id": "XvZNVMdGNEWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7: Evaluation with confidence threshold\n",
        "def predict(model, X, threshold=1.5):\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for s in X:\n",
        "            s = torch.FloatTensor(s).unsqueeze(0)\n",
        "            q = model(s)\n",
        "            # anomaly only if much stronger than normal\n",
        "            if q[0,1] > threshold * q[0,0]:\n",
        "                preds.append(1)\n",
        "            else:\n",
        "                preds.append(0)\n",
        "    return np.array(preds)\n",
        "\n",
        "y_pred = predict(model, X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSi-mGiLJ1n4",
        "outputId": "31ef1a3f-8c72-4532-ebfa-74981b29ea0e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1983\n",
            "           1       0.89      1.00      0.94        17\n",
            "\n",
            "    accuracy                           1.00      2000\n",
            "   macro avg       0.95      1.00      0.97      2000\n",
            "weighted avg       1.00      1.00      1.00      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1981    2]\n",
            " [   0   17]]\n"
          ]
        }
      ]
    }
  ]
}